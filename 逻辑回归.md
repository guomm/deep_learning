## 深度学习之Logistic Regression

线性回归的函数如下：
![](http://latex.codecogs.com/gif.latex?h%28x%29%20%3D%20wx%20%2B%20b)
逻辑回归则是通过对线性回归做次转换，来达到目的。其公式如下：
![](http://latex.codecogs.com/gif.latex?f%28x%29%3D%5Csigma%20%28h%28x%29%29)
###1、转换函数
为什么需要转换函数？
&emsp;&emsp;转换函数的主要作用是提供一种非线性的建模能力。如果没有转换函数，那么Logistic Regression就变成了仅能够表达线性映射的Linear Regression，此时即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。

转换函数的性质？
* 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。 
* 单调性： 当转换函数是单调的时候，单层网络能够保证是凸函数。 
* 输出值的范围： 当转换函数输出值是有限的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著;当转换函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate。

从目前来看，常见的转换函数多是分段线性和具有指数形状的非线性函数。常用的转换函数有三种：
* sigmoid
&emsp;&emsp;sigmoid函数是最常用的函数，将一个实数范围的值转化到[0,1]区间内。其公式如下：
![](http://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20%5Cfrac%20%7B%201%20%7D%7B%201%2B%7B%20e%20%7D%5E%7B%20-x%20%7D%20%7D%20)

* tanh
&emsp;&emsp;tanh函数和sigmoid函数很像，但是其范围是[-1,1]。其公式如下：
![](http://latex.codecogs.com/gif.latex?f%28x%29%20%3D%20%5Cfrac%20%7B%20%7B%20%20e%20%7D%5E%7B%20x%20%7D-%20%7B%20e%20%7D%5E%7B%20-x%20%7D%20%20%7D%7B%20%7B%20e%20%7D%5E%7B%20x%20%7D%20%2B%20%7B%20e%20%7D%5E%7B%20-x%20%7D%20%7D%20)

* relu
&emsp;&emsp;该函数主要是为了对抗梯度消失。也就是当梯度反向传播到第一层的时候，梯度容易趋近于0或者一个极小值。当x小于0时，其为0；当x大于等于0是，其为本身。公式如下：
![](http://latex.codecogs.com/gif.latex?f%28x%29%3Dmax%280%2Cx%29)

### 2、损失函数
#### 2.1、线性回归单变量损失函数
**线性回归单变量**的损失函数是所有点到线的距离（欧式距离）之和，我们只要找出最小化该距离的w和b即可：
![](http://latex.codecogs.com/gif.latex?%7B%20E%20%7D_%7B%20%28w%2Cb%29%20%7D%3D%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%28%7B%20%7B%20y%20%7D_%7B%20i%20%7D-w%7B%20x%20%7D_%7B%20i%20%7D-b%29%20%7D%5E%7B%202%20%7D%20%7D%20)
我们将E分别对w和b求导，得到：
![](http://latex.codecogs.com/gif.latex?%7B%20%5Cfrac%20%7B%20%5Cpartial%20%7B%20E%20%7D_%7B%20%28w%2Cb%29%20%7D%20%7D%7B%20%5Cpartial%20w%20%7D%20%20%7D%3D2%28w%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%7B%20x%20%7D_%7B%20i%20%7D%5E%7B%202%20%7D%20%7D%20-%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%28%7B%20%7B%20y%20%7D_%7B%20i%20%7D-b%20%7D%29%7B%20x%20%7D_%7B%20i%20%7D%29%20%7D%20)
![](http://latex.codecogs.com/gif.latex?%7B%20%5Cfrac%20%7B%20%5Cpartial%20%7B%20E%20%7D_%7B%20%28w%2Cb%29%20%7D%20%7D%7B%20%5Cpartial%20b%20%7D%20%20%7D%3D2%28mb-%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%28%7B%20%7B%20y%20%7D_%7B%20i%20%7D-w%20%7D%7B%20x%20%7D_%7B%20i%20%7D%29%20%7D%20%29)
另令上述两个式子为0，即可求出w和b的最优解：
![](http://latex.codecogs.com/gif.latex?%7B%20w%20%7D%3D%5Cfrac%20%7B%20%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%7B%20%7B%20y%20%7D_%7B%20i%20%7D%28%7B%20x%20%7D_%7B%20i%20%7D-%5Cfrac%20%7B%201%20%7D%7B%20m%20%7D%20%5Csum%20_%7B%20j%3D1%20%7D%5E%7B%20m%20%7D%7B%20%7B%20x%20%7D_%7B%20j%20%7D%20%7D%20%20%7D%29%20%7D%20%20%7D%7B%20%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%7B%20x%20%7D_%7B%20i%20%7D%5E%7B%202%20%7D%20%7D%20-%5Cfrac%20%7B%201%20%7D%7B%20m%20%7D%20%7B%20%28%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%7B%20x%20%7D_%7B%20i%20%7D%20%7D%20%29%20%7D%5E%7B%202%20%7D%20%7D%20)
![](http://latex.codecogs.com/gif.latex?%7B%20b%20%7D%3D%5Cfrac%20%7B%201%20%7D%7B%20m%20%7D%20%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%28%7B%20%7B%20y%20%7D_%7B%20i%20%7D-w%20%7D%7B%20x%20%7D_%7B%20i%20%7D%29%20%7D%20)
#### 2.1、线性回归多变量损失函数
更一般的情况是样本由d个属性描述，也就是**多元线性回归**。此时w是一个向量，我们可以把b放入w中统一考虑。那么此时的损失函数为：
![](http://latex.codecogs.com/gif.latex?%7B%20E%20%7D_%7B%20w%20%7D%3D%7B%20%28y-Xw%29%20%7D%5E%7B%20T%20%7D%28y-Xw%29)
将E对w求导：
![](http://latex.codecogs.com/gif.latex?%5Cfrac%20%7B%20%E2%88%82E%28w%29%20%7D%7B%20%E2%88%82w%20%7D%20%3D%7B%202X%20%7D%5E%7B%20T%20%7D%28Xw-y%29)
同样的，令上述式子为0可得：
![](http://latex.codecogs.com/gif.latex?w%3D%7B%20%5Cleft%28%20%7B%20X%20%7D%5E%7B%20T%20%7DX%20%5Cright%29%20%20%7D%5E%7B%20-1%20%7D%7B%20X%20%7D%5E%7B%20T%20%7Dy)
实际中{ X }^{ T }X经常不是可逆矩阵，此时可以得到多个w都可使得损失函数最小，选择哪个为输出由机器学习算法的归纳偏好决定。
#### 2.3、欧式距离做逻辑回归的损失函数？
啰啰嗦嗦讲了一大堆线性回归损失函数的一系列求解过程。如果我们用同样的求解欧式距离的方式作为逻辑回归的损失函数，行不行？
假设我们用sigmoid转换函数：
![](http://latex.codecogs.com/gif.latex?f%28x%29%3D%5Cfrac%20%7B%201%20%7D%7B%201%2B%7B%20e%20%7D%5E%7B%20-%28%7B%20w%20%7D%5E%7B%20T%20%7Dx%2Bb%29%20%7D%20%7D%20)
![](http://latex.codecogs.com/gif.latex?%7B%20E%20%7D_%7B%20%28w%2Cb%29%20%7D%3D%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%28%7B%20%7B%20y%20%7D_%7B%20i%20%7D-f%28%7B%20x%20%7D_%7B%20i%20%7D%29%29%20%7D%5E%7B%202%20%7D%20%7D%20)
由于f(x)是一个非线性函数，导致{ E }_{ (w,b) }不是一个凸函数，存在很多极小值点，我们并不能保证它会收敛到全局最小值。
#### 2.4、最大似然估计
既然欧式距离不行，那有没有其他的式子可以？在说出逻辑回归的损失函数之前，我们先聊下最大似然估计。
先举例解释下**最大似然估计**：假设有两个外形一样的箱子，第一个箱子中有99个白球和1个黑球，第二个箱子中有99个黑球和1个白球。从箱子中拿出一个白球，问这个白球是从那个箱子中拿出的？
人们的第一印象是该球最像从第一个箱子取出的，这个推断符合人们的经验认识。“最像”就是“最大似然”之意，这种想法就是最大似然原理。

最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。

原理：极大似然估计是建立在极大似然原理的基础上的一个统计方法，是概率论在统计学中的应用。极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。通过若干次试验，观察其结果，利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计。

<font color=red>重要前提：训练样本的分布能代表样本的真实分布。每个样本集中的样本都是所谓独立同分布的随机变量 (iid条件)，且有充分的训练样本。</font>

假设有个数据集：
![](http://latex.codecogs.com/gif.latex?D%3D%5Cleft%5C%7B%20%7B%20x%20%7D_%7B%201%20%7D%2C%7B%20x%20%7D_%7B%202%20%7D%2C%7B%20x%20%7D_%7B%203%20%7D%5Ccdots%20%7B%20x%20%7D_%7B%20N%20%7D%20%5Cright%5C%7D%20)
似然函数（linkehood function）：联合概率密度函数P(D|\theta )称为相对于D的θ的似然函数。
![](http://latex.codecogs.com/gif.latex?l%28%5Ctheta%20%29%3DP%28D%7C%5Ctheta%20%29%3DP%28%7B%20x%20%7D_%7B%201%20%7D%2C%7B%20x%20%7D_%7B%202%20%7D%5Ccdots%20%7B%20x%20%7D_%7B%20N%20%7D%7C%5Ctheta%20%29%3D%5Cprod%20_%7B%20i%3D1%20%7D%5E%7B%20N%20%7D%7B%20P%28%7B%20x%20%7D_%7B%20i%20%7D%7C%5Ctheta%20%29%20%7D%20)
极大似然估计就是求使得出现该组样本的概率最大的θ值：
![](http://latex.codecogs.com/gif.latex?%7B%20%5Ctheta%20%20%7D%3Darg%5Cunderset%20%7B%20%5Ctheta%20%20%7D%7B%20max%20%7D%20l%28%5Ctheta%20%29%3Darg%5Cunderset%20%7B%20%5Ctheta%20%20%7D%7B%20max%20%7D%20%5Cprod%20_%7B%20i%3D1%20%7D%5E%7B%20N%20%7D%7B%20P%28%7B%20x%20%7D_%7B%20i%20%7D%7C%5Ctheta%20%29%20%7D%20)
实际中为了便于分析，定义了对数似然函数：
![](http://latex.codecogs.com/gif.latex?%7B%20%5Ctheta%20%20%7D%3Darg%5Cunderset%20%7B%20%5Ctheta%20%20%7D%7B%20max%20%7D%20%5Cln%20%7B%20l%28%5Ctheta%20%29%20%7D%20)
#### 2.5、逻辑回归损失函数
我们知道逻辑回归的函数式如下：
![](http://latex.codecogs.com/gif.latex?y%3D%5Cfrac%20%7B%201%20%7D%7B%201%2B%7B%20e%20%7D%5E%7B%20-%28%7B%20w%20%7D%5E%7B%20T%20%7Dx%2Bb%29%20%7D%20%7D%20)
转化后为：
![](http://latex.codecogs.com/gif.latex?%5Cln%20%7B%20%5Cfrac%20%7B%20y%20%7D%7B%201-y%20%7D%20%20%7D%20%3D%7B%20w%20%7D%5E%7B%20T%20%7Dx%2Bb)
若将y视为类后验概率，则上述式子可重写为：
![](http://latex.codecogs.com/gif.latex?%5Cln%20%7B%20%5Cfrac%20%7B%20p%28y%3D1%7Cx%29%20%7D%7B%20p%28y%3D0%7Cx%29%20%7D%20%20%7D%20%3D%7B%20w%20%7D%5E%7B%20T%20%7Dx%2Bb)
于是我们可以通过极大似然法估计w和b。给定数据集，对率回归模型最大化对数似然估计：
![](http://latex.codecogs.com/gif.latex?l%28w%2Cb%29%3D%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%5Cln%20%7B%20p%28%7B%20y%20%7D_%7B%20i%20%7D%7C%7B%20x%20%7D_%7B%20i%20%7D%3Bw%2Cb%29%20%7D%20%20%7D%20)
即令每个样本属于其真实值标记的概率越大越好。令\beta =(w,b),\chi =(x;1)，则{ w }^{ T }x+b可简写为{ \beta  }^{ T }\chi 。对应到我们的逻辑回归则只有这两种情况{ p }_{ 1 }(\chi ;\beta )={ p }(y=1|\chi ;\beta )，{ p }_{ 0 }(\chi ;\beta )={ p }(y=0|\chi ;\beta )=1-{ p }_{ 1 }(\chi ;\beta )。则上述的似然项可重写为：
![](http://latex.codecogs.com/gif.latex?%7B%20p%28%7B%20y%20%7D_%7B%20i%20%7D%7C%7B%20x%20%7D_%7B%20i%20%7D%3Bw%2Cb%29%20%7D%3D%7B%20y%20%7D_%7B%20i%20%7D%7B%20p%20%7D_%7B%201%20%7D%28%7B%20%5Cchi%20%20%7D_%7B%20i%20%7D%3B%5Cbeta%20%29%2B%281-%7B%20y%20%7D_%7B%20i%20%7D%29%7B%20p%20%7D_%7B%200%20%7D%28%7B%20%5Cchi%20%20%7D_%7B%20i%20%7D%3B%5Cbeta%20%29)
因此最大化l(w,b)等价于最小化下面的式子：
![](http://latex.codecogs.com/gif.latex?cost%28w%2Cb%29%3D-%5Cleft%28%20%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20%7B%20y%20%7D_%7B%20i%20%7D%7B%20p%20%7D_%7B%201%20%7D%28%7B%20%5Cchi%20%20%7D_%7B%20i%20%7D%3B%5Cbeta%20%29%2B%281-%7B%20y%20%7D_%7B%20i%20%7D%29%7B%20p%20%7D_%7B%200%20%7D%28%7B%20%5Cchi%20%20%7D_%7B%20i%20%7D%3B%5Cbeta%20%29%20%7D%20%20%5Cright%29%20)
因此逻辑回归的损失函数如下：
![](http://latex.codecogs.com/gif.latex?cost%28f%28x%29%2Cy%29%3D%5Csum%20_%7B%20i%3D1%20%7D%5E%7B%20m%20%7D%7B%20-%7B%20y%20%7D_%7B%20i%20%7D%20%7D%20%5Clog%20%7B%20%28f%28%7B%20x%20%7D_%7B%20i%20%7D%29%20%7D%20%29-%281-y%29%5Clog%20%7B%20%281-f%28%7B%20x%20%7D_%7B%20i%20%7D%29%20%7D%20%29)

稍微解释下：当y=1时，如果此时f(x)=1,则单对这个样本而言的cost=0,表示这个样本的预测完全准确。那如果所有样本都预测准确，总的cost=0，达到了最优值。但是如果此时预测的概率f(x)=0，那么cost→∞，也就是此处的w和b对cost函数来说是一个很大的惩罚项。
当y=0时，推理过程跟上述完全一致，不再解释。
### 参考
1、https://blog.csdn.net/u014595019/article/details/52562159
2、https://blog.csdn.net/zengxiantao1994/article/details/72787849

